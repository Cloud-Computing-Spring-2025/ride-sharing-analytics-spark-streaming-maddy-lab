
# Real-Time Ride-Sharing Analytics with Apache Spark

This project demonstrates how to process and analyze real-time ride-sharing data using **Apache Spark Structured Streaming**. The data is ingested, parsed, aggregated, and analyzed in real-time to provide insights into driver performance and fare statistics.

## Tasks

### Task 1: Ingest and Parse Real-Time Ride Data
**Objective**: Ingest streaming data from a socket, parse it using a schema, and store it in CSV files.
- **Operations**:
  - Read streaming data from `localhost:9999`.
  - Define a schema to parse the JSON data.
  - Write parsed data to CSV files in `output/task1/parsed_data`.
- **Code**: 
  - Reads data from a socket stream.
  - Defines a schema for incoming ride data (`trip_id`, `driver_id`, `distance_km`, `fare_amount`, `timestamp`).
  - Writes parsed data to CSV files.

### Task 2: Real-Time Aggregations (Driver-Level)
**Objective**: Perform aggregations on the data, including the total fare and average distance per driver, and store the results.
- **Operations**:
  - Read CSV data produced in Task 1.
  - Perform real-time aggregation on `driver_id` to calculate total fare and average distance.
  - Save the results to CSV files in `output/task2`.
- **Code**: 
  - Reads CSV files generated by Task 1.
  - Aggregates data by `driver_id` using `sum` for fare and `avg` for distance.
  - Writes the aggregation results to CSV files.

### Task 3: Windowed Time-Based Analytics
**Objective**: Perform 5-minute sliding window aggregation on the total fare to analyze real-time trends.
- **Operations**:
  - Read CSV data from Task 1.
  - Perform a 5-minute windowed aggregation with a sliding window of 1 minute.
  - Write windowed results to CSV files in `output/task3`.
- **Code**: 
  - Uses watermarking to handle late-arriving data.
  - Aggregates the total fare over 5-minute windows, sliding every minute.
  - Saves the results to CSV using `foreachBatch`.

## Instructions

1. **Running the Tasks**:
  - **Task 1**: Start the `data_generator.py` to simulate ride data on `localhost:9999`.
  - Run the script:
    ```bash
    python task1.py
    ```
  - **Task 2**: Run the following to perform aggregations:
    ```bash
    python task2.py
    ```
  - **Task 3**: Run this to perform windowed aggregation:
    ```bash
    python task3.py
    ```

2. **Output**:
  - CSV files for each task will be generated in the respective `output` directories:
    - **Task 1**: `output/task1/parsed_data`
    - **Task 2**: `output/task2`
    - **Task 3**: `output/task3`

3. **Checkpointing**:
  - Spark checkpointing is used to store intermediate states and allow for recovery in case of failures:
    - **Task 1**: `output/task1/checkpoints`
    - **Task 2**: `output/task2/checkpoints`
    - **Task 3**: `output/task3/checkpoints`
